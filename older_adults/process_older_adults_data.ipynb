{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebe22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional SciPy (needed for quadratic/cubic + bandpass). Linear still works without it.\n",
    "try:\n",
    "    from scipy.interpolate import interp1d\n",
    "    from scipy.signal import butter, filtfilt\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class MergeConfig:\n",
    "    # EDA/TEMP mapping onto BVP grid\n",
    "    map_method: str = \"snap\"         # \"snap\" or \"interp\"\n",
    "    map_interp_kind: str = \"linear\"  # for EDA/TEMP if map_method=\"interp\"\n",
    "    map_snap_kind: str = \"one_to_one\"   # \"one_to_one\" or \"per_grid\"\n",
    "\n",
    "    # HR derivation / resampling\n",
    "    hr_target: str = \"64hz\"          # \"64hz\" or \"1hz\" (64hz is default)\n",
    "    hr_interp_kind: str = \"cubic\"    # \"linear\" / \"quadratic\" / \"cubic\"\n",
    "    hr_min_bpm: float = 30.0\n",
    "    hr_max_bpm: float = 220.0\n",
    "    hr_mad_z: float = 3.0            # robust spike rejection on instantaneous HR\n",
    "\n",
    "    # Default to lowpass on HR after resampling to 64 Hz (Nyquist = 32 Hz)\n",
    "    hr_bp_low_hz: float = 0.0\n",
    "    hr_bp_high_hz: float = 1.0\n",
    "    hr_bp_order: int = 2\n",
    "\n",
    "    # Section logic across segments\n",
    "    gap_threshold_s: float = 3.0     # default\n",
    "    fill_short_gaps: bool = True\n",
    "\n",
    "    # Which channels to interpolate across segment gaps (BVP usually should NOT)\n",
    "    gap_fill_channels: Tuple[str, ...] = (\"eda\", \"temperature\", \"hr\")\n",
    "\n",
    "    # Grid properties\n",
    "    fs_bvp: float = 64.0\n",
    "    extend_grid_to_union: bool = True  # extend beyond BVP span to cover earliest/latest among channels in that segment\n",
    "\n",
    "\n",
    "DT_NS_64 = int(round(1e9 / 64.0))  # 15_625_000 ns\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# IO helpers\n",
    "# ----------------------------\n",
    "def read_channel_csv(subject_dir: str, subject_id: str, channel: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Finds and reads one CSV for channel: *_{channel}.csv in subject_dir.\n",
    "    Expected columns: timestamp_ns, datetime_utc, segment, value (or peak).\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(subject_dir, f\"{subject_id}_{channel}.csv\")\n",
    "    if not os.path.exists(pattern):\n",
    "        # fallback: any *_channel.csv\n",
    "        candidates = glob.glob(os.path.join(subject_dir, f\"*_{channel}.csv\"))\n",
    "        if not candidates:\n",
    "            return None\n",
    "        pattern = sorted(candidates)[0]\n",
    "\n",
    "    df = pd.read_csv(pattern)\n",
    "    if \"timestamp_ns\" not in df.columns or \"segment\" not in df.columns:\n",
    "        raise ValueError(f\"{pattern} missing required columns (timestamp_ns, segment).\")\n",
    "\n",
    "    df[\"timestamp_ns\"] = df[\"timestamp_ns\"].astype(np.int64)\n",
    "    df[\"segment\"] = df[\"segment\"].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_sorted_unique_times(t_ns: np.ndarray, v: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sort by time and drop duplicate timestamps keeping the last.\"\"\"\n",
    "    order = np.argsort(t_ns, kind=\"mergesort\")\n",
    "    t = t_ns[order]\n",
    "    vv = v[order]\n",
    "    if t.size == 0:\n",
    "        return t, vv\n",
    "    # keep last occurrence\n",
    "    _, idx_last = np.unique(t, return_index=False), None\n",
    "    # implement keep-last by reversing unique\n",
    "    t_rev = t[::-1]\n",
    "    vv_rev = vv[::-1]\n",
    "    t_u_rev, uidx_rev = np.unique(t_rev, return_index=True)\n",
    "    keep_rev = uidx_rev\n",
    "    keep = (t.size - 1 - keep_rev)\n",
    "    keep.sort()\n",
    "    return t[keep], vv[keep]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Grid building\n",
    "# ----------------------------\n",
    "def build_bvp_native_grid(\n",
    "    bvp_seg: pd.DataFrame,\n",
    "    start_ns: Optional[int],\n",
    "    end_ns: Optional[int],\n",
    "    cfg: MergeConfig,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build per-segment 64Hz grid aligned to the segment's BVP phase.\n",
    "    - Native grid anchor is the first BVP timestamp in that segment.\n",
    "    - Optionally extends to [start_ns, end_ns] (union across channels).\n",
    "    \"\"\"\n",
    "    if bvp_seg is None or bvp_seg.empty:\n",
    "        raise ValueError(\"BVP segment is empty; cannot build native 64Hz grid.\")\n",
    "\n",
    "    bvp_ts = np.asarray(bvp_seg[\"timestamp_ns\"], dtype=np.int64)\n",
    "    bvp_ts = np.unique(bvp_ts)  # BVP should already be on-grid; keep unique\n",
    "    bvp_ts.sort()\n",
    "    anchor = int(bvp_ts[0])\n",
    "\n",
    "    # Default range: BVP range; optionally extend to union across channels\n",
    "    s = int(bvp_ts[0]) if start_ns is None else int(start_ns)\n",
    "    e = int(bvp_ts[-1]) if end_ns is None else int(end_ns)\n",
    "\n",
    "    if not cfg.extend_grid_to_union:\n",
    "        s, e = int(bvp_ts[0]), int(bvp_ts[-1])\n",
    "\n",
    "    # Align s,e to anchor phase so grid points fall on anchor + k*dt\n",
    "    if s <= anchor:\n",
    "        k_back = int(np.ceil((anchor - s) / DT_NS_64))\n",
    "        s_aligned = anchor - k_back * DT_NS_64\n",
    "    else:\n",
    "        k_fwd = int(np.floor((s - anchor) / DT_NS_64))\n",
    "        s_aligned = anchor + k_fwd * DT_NS_64\n",
    "\n",
    "    if e >= anchor:\n",
    "        k_end = int(np.floor((e - anchor) / DT_NS_64))\n",
    "        e_aligned = anchor + k_end * DT_NS_64\n",
    "    else:\n",
    "        k_end = int(-np.ceil((anchor - e) / DT_NS_64))\n",
    "        e_aligned = anchor + k_end * DT_NS_64\n",
    "\n",
    "    if e_aligned < s_aligned:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    n = int((e_aligned - s_aligned) // DT_NS_64) + 1\n",
    "    grid = s_aligned + np.arange(n, dtype=np.int64) * DT_NS_64\n",
    "    return grid\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Mapping EDA/TEMP to grid\n",
    "# ----------------------------\n",
    "def nearest_neighbor_on_grid(\n",
    "    t_s: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    t_g: np.ndarray,\n",
    "    mode: str = \"one_to_one\",   # \"one_to_one\" or \"per_grid\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nearest-neighbor snapping between sample times t_s and grid times t_g.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_s : sorted sample timestamps (ns)\n",
    "    v   : sample values\n",
    "    t_g : sorted grid timestamps (ns)\n",
    "    mode:\n",
    "      - \"per_grid\": for each grid time, take value of nearest sample time.\n",
    "                    A sample may be reused for many grid points.\n",
    "      - \"one_to_one\": each sample is snapped to ONLY ONE nearest grid time (its nearest gridline).\n",
    "                      Each sample used at most once; grid points without an assigned sample are NaN.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : array aligned to t_g\n",
    "    \"\"\"\n",
    "    t_s = np.asarray(t_s, dtype=np.int64)\n",
    "    t_g = np.asarray(t_g, dtype=np.int64)\n",
    "    v = np.asarray(v, dtype=np.float64)\n",
    "\n",
    "    out = np.full(t_g.shape, np.nan, dtype=np.float64)\n",
    "    if t_s.size == 0 or t_g.size == 0:\n",
    "        return out\n",
    "\n",
    "    if mode == \"per_grid\":\n",
    "        # assume t_s sorted\n",
    "        idx = np.searchsorted(t_s, t_g, side=\"left\")\n",
    "        idx0 = np.clip(idx - 1, 0, t_s.size - 1)\n",
    "        idx1 = np.clip(idx, 0, t_s.size - 1)\n",
    "\n",
    "        d0 = np.abs(t_g - t_s[idx0])\n",
    "        d1 = np.abs(t_g - t_s[idx1])\n",
    "        pick = np.where(d1 < d0, idx1, idx0)\n",
    "        return v[pick].astype(np.float64)\n",
    "\n",
    "    if mode == \"one_to_one\":\n",
    "        # For each sample, find its nearest grid index\n",
    "        j = np.searchsorted(t_g, t_s, side=\"left\")\n",
    "        j0 = np.clip(j - 1, 0, t_g.size - 1)\n",
    "        j1 = np.clip(j, 0, t_g.size - 1)\n",
    "\n",
    "        d0 = np.abs(t_s - t_g[j0])\n",
    "        d1 = np.abs(t_s - t_g[j1])\n",
    "        j_pick = np.where(d1 < d0, j1, j0)\n",
    "\n",
    "        # If multiple samples snap to same grid index, keep the closest one;\n",
    "        # if tie, keep the later sample (arbitrary but deterministic).\n",
    "        # We'll resolve by sorting by (grid_index, distance, sample_time) then taking first per group.\n",
    "        dist = np.abs(t_s - t_g[j_pick]).astype(np.int64)\n",
    "        order = np.lexsort((-t_s, dist, j_pick))  # grid asc, dist asc, time desc\n",
    "        j_pick_ord = j_pick[order]\n",
    "        v_ord = v[order]\n",
    "        dist_ord = dist[order]\n",
    "\n",
    "        # unique per grid index: take first occurrence in sorted order\n",
    "        _, first_idx = np.unique(j_pick_ord, return_index=True)\n",
    "        chosen = first_idx\n",
    "        out[j_pick_ord[chosen]] = v_ord[chosen]\n",
    "        return out\n",
    "\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "\n",
    "def interp_on_grid(t_s: np.ndarray, v: np.ndarray, t_g: np.ndarray, kind: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Robust interpolation to grid:\n",
    "      - uses relative seconds to avoid huge ns magnitudes\n",
    "      - removes duplicate timestamps (keep last)\n",
    "      - degrades cubic/quadratic -> linear if insufficient points\n",
    "      - (optional) no extrapolation: fills outside-range with NaN\n",
    "    \"\"\"\n",
    "    if t_s.size < 2:\n",
    "        return np.full(t_g.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    # sort + keep-last on duplicates\n",
    "    t_s, v = ensure_sorted_unique_times(t_s.astype(np.int64), v.astype(np.float64))\n",
    "\n",
    "    # finite mask\n",
    "    m = np.isfinite(v)\n",
    "    if m.sum() < 2:\n",
    "        return np.full(t_g.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    t_s = t_s[m]\n",
    "    v = v[m]\n",
    "\n",
    "    # interpolate in relative seconds for numerical stability\n",
    "    t0 = int(t_s[0])\n",
    "    xs = (t_s - t0).astype(np.float64) / 1e9\n",
    "    xg = (t_g.astype(np.int64) - t0).astype(np.float64) / 1e9\n",
    "\n",
    "    # decide effective kind based on available points & SciPy\n",
    "    eff_kind = kind\n",
    "    if (not SCIPY_OK) and eff_kind in (\"quadratic\", \"cubic\"):\n",
    "        eff_kind = \"linear\"\n",
    "\n",
    "    if eff_kind == \"cubic\":\n",
    "        if xs.size < 4:\n",
    "            eff_kind = \"quadratic\" if (SCIPY_OK and xs.size >= 3) else \"linear\"\n",
    "    elif eff_kind == \"quadratic\":\n",
    "        if xs.size < 3:\n",
    "            eff_kind = \"linear\"\n",
    "\n",
    "    # linear path (no SciPy required)\n",
    "    if eff_kind == \"linear\" or (not SCIPY_OK):\n",
    "        return np.interp(xg, xs, v, left=np.nan, right=np.nan).astype(np.float64)\n",
    "\n",
    "    # SciPy quadratic/cubic\n",
    "    f = interp1d(\n",
    "        xs,\n",
    "        v,\n",
    "        kind=eff_kind,\n",
    "        bounds_error=False,\n",
    "        fill_value=np.nan, # or \"extrapolate\": BEWARE OF HUGE ARTIFACTS\n",
    "        assume_sorted=True,\n",
    "    )\n",
    "    return f(xg).astype(np.float64)\n",
    "\n",
    "\n",
    "\n",
    "def map_scalar_channel_to_grid(\n",
    "    df_seg: Optional[pd.DataFrame],\n",
    "    grid_ts: np.ndarray,\n",
    "    method: str,\n",
    "    interp_kind: str,\n",
    "    snap_kind:str,\n",
    ") -> np.ndarray:\n",
    "    if df_seg is None or df_seg.empty:\n",
    "        return np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "    t = np.asarray(df_seg[\"timestamp_ns\"], dtype=np.int64)\n",
    "    v = np.asarray(df_seg[\"value\"], dtype=np.float64)\n",
    "    t, v = ensure_sorted_unique_times(t, v)\n",
    "\n",
    "    if method == \"snap\":\n",
    "        return nearest_neighbor_on_grid(t, v, grid_ts, snap_kind)\n",
    "    elif method == \"interp\":\n",
    "        return interp_on_grid(t, v, grid_ts, kind=interp_kind)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mapping method: {method}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Peaks -> HR\n",
    "# ----------------------------\n",
    "def robust_filter_hr(t_ns: np.ndarray, hr: np.ndarray, cfg: MergeConfig) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Outlier filter on instantaneous HR:\n",
    "      - hard bpm bounds\n",
    "      - MAD-based spike rejection\n",
    "    \"\"\"\n",
    "    m = np.isfinite(hr)\n",
    "    m &= (hr >= cfg.hr_min_bpm) & (hr <= cfg.hr_max_bpm)\n",
    "    t_ns = t_ns[m]\n",
    "    hr = hr[m]\n",
    "    if hr.size < 5:\n",
    "        return t_ns, hr\n",
    "\n",
    "    med = np.median(hr)\n",
    "    mad = np.median(np.abs(hr - med))\n",
    "    if mad <= 1e-9:\n",
    "        return t_ns, hr\n",
    "    z = np.abs(hr - med) / (1.4826 * mad) # z score of MAD\n",
    "    keep = z <= cfg.hr_mad_z\n",
    "    return t_ns[keep], hr[keep]\n",
    "\n",
    "\n",
    "def bandpass_filter_64hz(x: np.ndarray, cfg: MergeConfig) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Baseline-preserving Butterworth filtering on 64Hz HR.\n",
    "    If low=0 and high>0 -> lowpass smoothing.\n",
    "    If low>0 and high>0 -> bandpass of deviations, then baseline added back.\n",
    "    \"\"\"\n",
    "    if not SCIPY_OK:\n",
    "        return x\n",
    "\n",
    "    fs = 64.0\n",
    "    low = float(cfg.hr_bp_low_hz)\n",
    "    high = float(cfg.hr_bp_high_hz)\n",
    "\n",
    "    # If both are 0, filtering disabled\n",
    "    if (low <= 0) and (high <= 0):\n",
    "        return x\n",
    "\n",
    "    nyq = fs / 2\n",
    "    if high > 0 and high >= nyq:\n",
    "        high = 0.99 * nyq\n",
    "\n",
    "    # Choose filter type\n",
    "    if low <= 0 and high > 0:\n",
    "        btype = \"lowpass\"\n",
    "        Wn = high / nyq\n",
    "    elif high <= 0 and low > 0:\n",
    "        btype = \"highpass\"\n",
    "        Wn = low / nyq\n",
    "    else:\n",
    "        btype = \"bandpass\"\n",
    "        Wn = [low / nyq, high / nyq]\n",
    "\n",
    "    b, a = butter(cfg.hr_bp_order, Wn, btype=btype)\n",
    "\n",
    "    y = x.astype(np.float64).copy()\n",
    "    m = np.isfinite(y)\n",
    "    if m.sum() < 10:\n",
    "        return y\n",
    "\n",
    "    # Baseline to preserve absolute HR scale\n",
    "    baseline = np.nanmedian(y)\n",
    "    y[m] = y[m] - baseline\n",
    "\n",
    "    # Filter contiguous finite runs\n",
    "    idx = np.where(m)[0]\n",
    "    runs = np.split(idx, np.where(np.diff(idx) != 1)[0] + 1)\n",
    "    for r in runs:\n",
    "        if r.size < max(10, 3 * (cfg.hr_bp_order + 1)):\n",
    "            continue\n",
    "        y[r] = filtfilt(b, a, y[r])\n",
    "\n",
    "    y[m] = y[m] + baseline\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def derive_hr_on_grid(\n",
    "    peaks_seg: Optional[pd.DataFrame],\n",
    "    grid_ts: np.ndarray,\n",
    "    cfg: MergeConfig,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    peaks -> IBI -> HR:\n",
    "      1) instantaneous HR at beat times\n",
    "      2) outlier filter\n",
    "      3) resample to 1Hz grid (cubic default)\n",
    "      4) upsample/interp to 64Hz BVP grid (default)\n",
    "      5) bandpass filter at fs=64 (Nyquist=32)\n",
    "    \"\"\"\n",
    "    if peaks_seg is None or peaks_seg.empty:\n",
    "        return np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    # peaks timestamps\n",
    "    t_peaks = np.asarray(peaks_seg[\"timestamp_ns\"], dtype=np.int64)\n",
    "    t_peaks = np.unique(t_peaks)\n",
    "    t_peaks.sort()\n",
    "    if t_peaks.size < 3:\n",
    "        return np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    ibi_s = np.diff(t_peaks).astype(np.float64) / 1e9\n",
    "    # associate HR with the later peak time\n",
    "    t_hr_inst = t_peaks[1:]\n",
    "    hr_inst = 60.0 / np.maximum(ibi_s, 1e-9)\n",
    "\n",
    "    t_hr_inst, hr_inst = robust_filter_hr(t_hr_inst, hr_inst, cfg)\n",
    "    if hr_inst.size < 3:\n",
    "        return np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    # Build a 1Hz grid within the segment span (in ns)\n",
    "    seg_start = int(grid_ts[0])\n",
    "    seg_end = int(grid_ts[-1])\n",
    "    start_s = int(np.ceil(seg_start / 1e9))\n",
    "    end_s = int(np.floor(seg_end / 1e9))\n",
    "    if end_s <= start_s:\n",
    "        return np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "\n",
    "    t1_ns = (np.arange(start_s, end_s + 1, dtype=np.int64) * 1_000_000_000)\n",
    "    #t1_ns = (np.arange(start_s, (end_s + 1)*4, dtype=np.int64) * 250000000)\n",
    "\n",
    "    # Interpolate instantaneous HR onto 1Hz grid\n",
    "    kind = cfg.hr_interp_kind\n",
    "    # If SciPy not available, quadratic/cubic will fall back to linear inside interp_on_grid\n",
    "    hr_1 = interp_on_grid(t_hr_inst, hr_inst, t1_ns, kind=kind)\n",
    "\n",
    "    if cfg.hr_target == \"1hz\":\n",
    "        # If a 1Hz dataset is needed, return HR broadcast to 64Hz grid by nearest-neighbor.\n",
    "        # (Still keeps final merged DF at 64Hz.)\n",
    "        hr_64 = nearest_neighbor_on_grid(t1_ns.astype(np.int64), hr_1.astype(np.float64), grid_ts, cfg.map_snap_kind)\n",
    "        hr_64 = bandpass_filter_64hz(hr_64, cfg)\n",
    "        return hr_64\n",
    "\n",
    "    # Default: interpolate 1Hz HR to 64Hz grid\n",
    "    hr_64 = interp_on_grid(t1_ns.astype(np.int64), hr_1.astype(np.float64), grid_ts, kind=kind)\n",
    "    hr_64 = bandpass_filter_64hz(hr_64, cfg)\n",
    "    return hr_64\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Per-segment merge\n",
    "# ----------------------------\n",
    "def segment_union_bounds(\n",
    "    bvp_seg: Optional[pd.DataFrame],\n",
    "    eda_seg: Optional[pd.DataFrame],\n",
    "    temp_seg: Optional[pd.DataFrame],\n",
    "    peaks_seg: Optional[pd.DataFrame],\n",
    ") -> Tuple[Optional[int], Optional[int]]:\n",
    "    \"\"\"\n",
    "    Return earliest and latest timestamps among available channels for this segment.\n",
    "    \"\"\"\n",
    "    tmins = []\n",
    "    tmaxs = []\n",
    "    for df, col in [(bvp_seg, \"timestamp_ns\"), (eda_seg, \"timestamp_ns\"), (temp_seg, \"timestamp_ns\"), (peaks_seg, \"timestamp_ns\")]:\n",
    "        if df is not None and not df.empty:\n",
    "            tmins.append(int(df[col].min()))\n",
    "            tmaxs.append(int(df[col].max()))\n",
    "    if not tmins:\n",
    "        return None, None\n",
    "    return min(tmins), max(tmaxs)\n",
    "\n",
    "\n",
    "def build_per_segment_df(\n",
    "    seg_id: int,\n",
    "    bvp_seg: pd.DataFrame,\n",
    "    eda_seg: Optional[pd.DataFrame],\n",
    "    temp_seg: Optional[pd.DataFrame],\n",
    "    peaks_seg: Optional[pd.DataFrame],\n",
    "    cfg: MergeConfig,\n",
    ") -> pd.DataFrame:\n",
    "    # compute union bounds (optional grid extension)\n",
    "    start_ns, end_ns = segment_union_bounds(bvp_seg, eda_seg, temp_seg, peaks_seg)\n",
    "    grid_ts = build_bvp_native_grid(bvp_seg, start_ns, end_ns, cfg)\n",
    "    if grid_ts.size == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # BVP on grid: exact placement; if slight mismatch, fall back to nearest\n",
    "    bvp_t = np.asarray(bvp_seg[\"timestamp_ns\"], dtype=np.int64)\n",
    "    bvp_v = np.asarray(bvp_seg[\"value\"], dtype=np.float64)\n",
    "    bvp_t, bvp_v = ensure_sorted_unique_times(bvp_t, bvp_v)\n",
    "\n",
    "    # Try direct reindex-like mapping (fast path)\n",
    "    bvp_map = np.full(grid_ts.shape, np.nan, dtype=np.float64)\n",
    "    pos = np.searchsorted(grid_ts, bvp_t)\n",
    "    ok = (pos >= 0) & (pos < grid_ts.size) & (grid_ts[pos] == bvp_t)\n",
    "    bvp_map[pos[ok]] = bvp_v[ok]\n",
    "    # if too sparse (meaning timestamps didnâ€™t match), use nearest neighbor\n",
    "    if np.isfinite(bvp_map).sum() < 0.9 * min(grid_ts.size, bvp_t.size):\n",
    "        bvp_map = nearest_neighbor_on_grid(bvp_t, bvp_v, grid_ts)\n",
    "\n",
    "    eda_map = map_scalar_channel_to_grid(eda_seg, grid_ts, method=cfg.map_method, interp_kind=cfg.map_interp_kind, snap_kind=cfg.map_snap_kind)\n",
    "    temp_map = map_scalar_channel_to_grid(temp_seg, grid_ts, method=cfg.map_method, interp_kind=cfg.map_interp_kind, snap_kind=cfg.map_snap_kind)\n",
    "\n",
    "    hr_map = derive_hr_on_grid(peaks_seg, grid_ts, cfg)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"timestamp_ns\": grid_ts,\n",
    "        \"datetime_utc\": pd.to_datetime(grid_ts, unit=\"ns\", utc=True),\n",
    "        \"segment\": seg_id,\n",
    "        \"bvp\": bvp_map,\n",
    "        \"eda\": eda_map,\n",
    "        \"temperature\": temp_map,\n",
    "        \"hr\": hr_map,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Across-segment: sectioning + optional gap fill\n",
    "# ----------------------------\n",
    "def compute_gap_s(prev_end_ns: int, next_start_ns: int) -> float:\n",
    "    \"\"\"\n",
    "    Gap in seconds between two 64Hz grids accounting for one dt step.\n",
    "    If next starts exactly one dt after prev ends, gap=0.\n",
    "    \"\"\"\n",
    "    raw = (next_start_ns - prev_end_ns - DT_NS_64) / 1e9\n",
    "    return float(max(0.0, raw))\n",
    "\n",
    "\n",
    "def make_gap_rows(prev_end_ns: int, next_start_ns: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create missing 64Hz timestamps continuing prev grid phase:\n",
    "      (prev_end + dt) ... (next_start - dt)\n",
    "    \"\"\"\n",
    "    if next_start_ns <= prev_end_ns + DT_NS_64:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    n = int((next_start_ns - prev_end_ns) // DT_NS_64) - 1\n",
    "    if n <= 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "    return prev_end_ns + np.arange(1, n + 1, dtype=np.int64) * DT_NS_64\n",
    "\n",
    "\n",
    "def assign_sections_and_merge(segments: List[pd.DataFrame], cfg: MergeConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - sort segments by start time\n",
    "    - assign section ids based on gap_threshold_s\n",
    "    - for short gaps: insert gap rows (NaNs) and interpolate specified channels across them\n",
    "    - do NOT force a master macro-grid across segments; keeps each segment's timestamps as-is\n",
    "    \"\"\"\n",
    "    seg_info = []\n",
    "    for df in segments:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        seg_id = int(df[\"segment\"].iloc[0])\n",
    "        seg_info.append((int(df[\"timestamp_ns\"].iloc[0]), int(df[\"timestamp_ns\"].iloc[-1]), seg_id, df))\n",
    "\n",
    "    seg_info.sort(key=lambda x: x[0])\n",
    "\n",
    "    out_parts = []\n",
    "    section_id = 0\n",
    "\n",
    "    for i, (s0, e0, seg_id, df) in enumerate(seg_info):\n",
    "        if i == 0:\n",
    "            df2 = df.copy()\n",
    "            df2[\"section\"] = section_id\n",
    "            out_parts.append(df2)\n",
    "            continue\n",
    "\n",
    "        prev_df = out_parts[-1]\n",
    "        prev_end = int(prev_df[\"timestamp_ns\"].iloc[-1])\n",
    "        gap_s = compute_gap_s(prev_end, s0)\n",
    "\n",
    "        # overlap handling: if overlap, we keep later segment data on duplicates (drop later duplicates after concat)\n",
    "        same_section = gap_s <= cfg.gap_threshold_s\n",
    "\n",
    "        if not same_section:\n",
    "            section_id += 1\n",
    "            df2 = df.copy()\n",
    "            df2[\"section\"] = section_id\n",
    "            out_parts.append(df2)\n",
    "            continue\n",
    "\n",
    "        # same section: optionally fill small gaps by adding rows continuing prev grid\n",
    "        if cfg.fill_short_gaps:\n",
    "            gap_ts = make_gap_rows(prev_end, s0)\n",
    "            if gap_ts.size > 0:\n",
    "                gap_block = pd.DataFrame({\n",
    "                    \"timestamp_ns\": gap_ts,\n",
    "                    \"datetime_utc\": pd.to_datetime(gap_ts, unit=\"ns\", utc=True),\n",
    "                    \"segment\": -1,              # gap rows are synthetic\n",
    "                    \"bvp\": np.nan,\n",
    "                    \"eda\": np.nan,\n",
    "                    \"temperature\": np.nan,\n",
    "                    \"hr\": np.nan,\n",
    "                    \"section\": section_id,\n",
    "                })\n",
    "                out_parts.append(gap_block)\n",
    "\n",
    "        df2 = df.copy()\n",
    "        df2[\"section\"] = section_id\n",
    "        out_parts.append(df2)\n",
    "\n",
    "    merged = pd.concat(out_parts, ignore_index=True)\n",
    "    merged = merged.sort_values(\"timestamp_ns\", kind=\"mergesort\")\n",
    "\n",
    "    # Resolve duplicates (overlaps) by keeping last\n",
    "    merged = merged.drop_duplicates(subset=[\"timestamp_ns\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "    # Interpolate across inserted gap rows *within each section* for selected channels only\n",
    "    if cfg.fill_short_gaps and cfg.gap_fill_channels and cfg.map_snap_kind != \"one_to_one\":\n",
    "        merged = merged.set_index(\"timestamp_ns\")\n",
    "        for sec, g in merged.groupby(\"section\", sort=False):\n",
    "            # interpolate only inside; do not extrapolate ends\n",
    "            for ch in cfg.gap_fill_channels:\n",
    "                if ch in g.columns:\n",
    "                    merged.loc[g.index, ch] = g[ch].interpolate(method=\"index\", limit_area=\"inside\")\n",
    "        merged = merged.reset_index()\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# End-to-end per subject\n",
    "# ----------------------------\n",
    "def process_subject(subject_dir: str, subject_id: str, out_path: str, cfg: MergeConfig) -> pd.DataFrame:\n",
    "    bvp = read_channel_csv(subject_dir, subject_id, \"bvp\")\n",
    "    eda = read_channel_csv(subject_dir, subject_id, \"eda\")\n",
    "    temp = read_channel_csv(subject_dir, subject_id, \"temperature\")\n",
    "    peaks = read_channel_csv(subject_dir, subject_id, \"systolicPeaks\")\n",
    "\n",
    "    if bvp is None or bvp.empty:\n",
    "        raise ValueError(f\"Missing/empty BVP for subject {subject_id} in {subject_dir}\")\n",
    "\n",
    "    # group by segment\n",
    "    seg_ids = sorted(bvp[\"segment\"].unique().tolist())\n",
    "\n",
    "    segments_merged = []\n",
    "    for seg_id in seg_ids:\n",
    "        bvp_seg = bvp[bvp[\"segment\"] == seg_id]\n",
    "        eda_seg = eda[eda[\"segment\"] == seg_id] if eda is not None else None\n",
    "        temp_seg = temp[temp[\"segment\"] == seg_id] if temp is not None else None\n",
    "        peaks_seg = peaks[peaks[\"segment\"] == seg_id] if peaks is not None else None\n",
    "\n",
    "        df_seg = build_per_segment_df(seg_id, bvp_seg, eda_seg, temp_seg, peaks_seg, cfg)\n",
    "        if not df_seg.empty:\n",
    "            segments_merged.append(df_seg)\n",
    "\n",
    "    merged = assign_sections_and_merge(segments_merged, cfg)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def process_all_subjects(root: str, combined_out_dir: str, cfg: MergeConfig):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      root/\n",
    "        1/  (subject folder)\n",
    "          1_bvp.csv, 1_eda.csv, 1_temperature.csv, 1_systolicPeaks.csv\n",
    "        2/\n",
    "          2_bvp.csv, ...\n",
    "      Writes:\n",
    "        combined_out_dir/1_merged_64hz.csv, ...\n",
    "    \"\"\"\n",
    "    subj_dirs = [d for d in glob.glob(os.path.join(root, \"*\")) if os.path.isdir(d)]\n",
    "    subj_dirs.sort()\n",
    "\n",
    "    for sd in subj_dirs:\n",
    "        subject_id = os.path.basename(sd)\n",
    "        out_path = os.path.join(combined_out_dir, f\"{subject_id}_merged_64hz.csv\")\n",
    "        print(f\"[SUBJECT {subject_id}] -> {out_path}\")\n",
    "        process_subject(sd, subject_id, out_path, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebae44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJECT 001] -> ./processed_data/per_subject/001_merged_64hz.csv\n",
      "[SUBJECT 002] -> ./processed_data/per_subject/002_merged_64hz.csv\n",
      "[SUBJECT 003] -> ./processed_data/per_subject/003_merged_64hz.csv\n",
      "[SUBJECT 004] -> ./processed_data/per_subject/004_merged_64hz.csv\n",
      "[SUBJECT 005] -> ./processed_data/per_subject/005_merged_64hz.csv\n",
      "[SUBJECT 006] -> ./processed_data/per_subject/006_merged_64hz.csv\n",
      "[SUBJECT 007] -> ./processed_data/per_subject/007_merged_64hz.csv\n"
     ]
    }
   ],
   "source": [
    "cfg = MergeConfig(\n",
    "    # all are default parameter values; explicitly specified here for readability\n",
    "    gap_threshold_s=3.0,\n",
    "    map_method=\"snap\",\n",
    "    map_interp_kind=\"linear\",\n",
    "    hr_target=\"1hz\",\n",
    "    hr_interp_kind=\"linear\",\n",
    "    hr_bp_low_hz=0,\n",
    "    hr_bp_high_hz=1.0\n",
    ")\n",
    "\n",
    "if (cfg.map_method == \"interp\" or cfg.hr_interp_kind in (\"quadratic\", \"cubic\")) and not SCIPY_OK:\n",
    "    print(\"[WARN] SciPy not available: quadratic/cubic will fall back to linear; bandpass disabled.\")\n",
    "\n",
    "process_all_subjects('./extraction_from_original_files', './processed_data/per_subject', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    #   python merge_physio.py --output_root ./output --combined_out ./processed_data/combined\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output_root\", required=True, help=\"Root dir containing subject folders (1..7)\")\n",
    "    parser.add_argument(\"--combined_out\", required=True, help=\"Dir to write merged outputs\")\n",
    "    parser.add_argument(\"--gap_threshold_s\", type=float, default=3.0)\n",
    "    parser.add_argument(\"--map_method\", choices=[\"snap\", \"interp\"], default=\"snap\")\n",
    "    parser.add_argument(\"--map_interp_kind\", choices=[\"linear\", \"quadratic\", \"cubic\"], default=\"linear\")\n",
    "    parser.add_argument(\"--hr_interp_kind\", choices=[\"linear\", \"quadratic\", \"cubic\"], default=\"cubic\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg = MergeConfig(\n",
    "        gap_threshold_s=args.gap_threshold_s,\n",
    "        map_method=args.map_method,\n",
    "        map_interp_kind=args.map_interp_kind,\n",
    "        hr_interp_kind=args.hr_interp_kind,\n",
    "    )\n",
    "\n",
    "    if (cfg.map_method == \"interp\" or cfg.hr_interp_kind in (\"quadratic\", \"cubic\")) and not SCIPY_OK:\n",
    "        print(\"[WARN] SciPy not available: quadratic/cubic will fall back to linear; bandpass disabled.\")\n",
    "\n",
    "    process_all_subjects(args.output_root, args.combined_out, cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
