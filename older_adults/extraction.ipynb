{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690ef1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d7ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# helpers: time unit + parsing\n",
    "# ----------------------------\n",
    "def to_epoch_ns(ts: int) -> int:\n",
    "    \"\"\"\n",
    "    Heuristic conversion to epoch nanoseconds.\n",
    "      - peaksTimeNanos: nanoseconds (≈1e18)\n",
    "      - timestampStart: often microseconds (≈1e15)\n",
    "    \"\"\"\n",
    "    ts = int(ts)\n",
    "    if ts >= 10**17:      # ns\n",
    "        return ts\n",
    "    if ts >= 10**14:      # us\n",
    "        return ts * 1_000\n",
    "    if ts >= 10**11:      # ms\n",
    "        return ts * 1_000_000\n",
    "    return ts * 1_000_000_000  # seconds\n",
    "\n",
    "\n",
    "def segment_to_df(seg: dict, signal_name: str, segment_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a sampled segment dict:\n",
    "      {'timestampStart': ..., 'samplingFrequency': ..., 'values': [...]}\n",
    "    into DataFrame with timestamp_ns + signal columns + segment.\n",
    "    \"\"\"\n",
    "    ts0_ns = to_epoch_ns(seg[\"timestampStart\"])\n",
    "    fs = float(seg[\"samplingFrequency\"])\n",
    "    values = seg.get(\"values\", [])\n",
    "    if not values:\n",
    "        return pd.DataFrame(columns=[\"timestamp_ns\", \"segment\"])\n",
    "\n",
    "    n = len(values)\n",
    "    offsets_ns = np.rint((np.arange(n, dtype=np.float64) / fs) * 1e9).astype(np.int64)\n",
    "    t_ns = ts0_ns + offsets_ns\n",
    "\n",
    "    first = values[0]\n",
    "\n",
    "    # Case A: values are dicts like {\"x\":..,\"y\":..,\"z\":..}\n",
    "    if isinstance(first, dict):\n",
    "        cols = sorted(first.keys())\n",
    "        data = {c: [v.get(c, np.nan) if isinstance(v, dict) else np.nan for v in values] for c in cols}\n",
    "        df = pd.DataFrame(data)\n",
    "        df.insert(0, \"timestamp_ns\", t_ns)\n",
    "        df.insert(1, \"segment\", segment_id)\n",
    "        return df\n",
    "\n",
    "    # Case B: values are vectors like [x,y,z]\n",
    "    if isinstance(first, (list, tuple, np.ndarray)) and len(first) in (2, 3, 4):\n",
    "        k = len(first)\n",
    "        mat = np.asarray(values, dtype=np.float64).reshape(n, k)\n",
    "        colnames = [\"c0\", \"c1\", \"c2\", \"c3\"][:k]\n",
    "        df = pd.DataFrame(mat, columns=colnames)\n",
    "        df.insert(0, \"timestamp_ns\", t_ns)\n",
    "        df.insert(1, \"segment\", segment_id)\n",
    "        return df\n",
    "\n",
    "    # Case C: scalar values\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp_ns\": t_ns,\n",
    "        \"segment\": segment_id,\n",
    "        \"value\": pd.to_numeric(values, errors=\"coerce\")\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "def peaks_to_df(peaks_time_nanos, segment_id: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert peaksTimeNanos list into DataFrame with timestamp_ns + peak flag + segment.\n",
    "    \"\"\"\n",
    "    if not peaks_time_nanos:\n",
    "        return pd.DataFrame(columns=[\"timestamp_ns\", \"segment\", \"peak\"])\n",
    "    t_ns = np.asarray(peaks_time_nanos, dtype=np.int64)\n",
    "    df = pd.DataFrame({\"timestamp_ns\": t_ns, \"peak\": 1})\n",
    "    df.insert(1, \"segment\", segment_id)\n",
    "    return df\n",
    "\n",
    "\n",
    "def iter_avro_records(avro_path: str):\n",
    "    \"\"\"Yield each record in the avro file (don’t assume only one).\"\"\"\n",
    "    reader = None\n",
    "    try:\n",
    "        reader = DataFileReader(open(avro_path, \"rb\"), DatumReader())\n",
    "        for rec in reader:\n",
    "            yield rec\n",
    "    finally:\n",
    "        if reader is not None:\n",
    "            reader.close()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Part 1 (DONE): parse folder -> per-channel DataFrames (with segment id)\n",
    "# ----------------------------\n",
    "def parse_folder_to_channel_dfs(folder: str, keys=None):\n",
    "    \"\"\"\n",
    "    Returns: dict(signal_name -> DataFrame)\n",
    "    Each DF has: timestamp_ns, datetime_utc, segment, and signal columns.\n",
    "    \"\"\"\n",
    "    if keys is None:\n",
    "        keys = [\"accelerometer\", \"eda\", \"temperature\", \"bvp\", \"systolicPeaks\"]\n",
    "\n",
    "    avro_paths = sorted(glob.glob(os.path.join(folder, \"*.avro\")))\n",
    "    buckets = {k: [] for k in keys}\n",
    "\n",
    "    for segment_id, avro_path in enumerate(avro_paths):\n",
    "        for rec in iter_avro_records(avro_path):\n",
    "            raw = rec.get(\"rawData\", {}) or {}\n",
    "\n",
    "            for k in keys:\n",
    "                if k not in raw or raw[k] is None:\n",
    "                    continue\n",
    "\n",
    "                payload = raw[k]\n",
    "\n",
    "                if k == \"systolicPeaks\":\n",
    "                    segments = payload if isinstance(payload, list) else [payload]\n",
    "                    for seg in segments:\n",
    "                        if isinstance(seg, dict):\n",
    "                            df = peaks_to_df(seg.get(\"peaksTimeNanos\", []), segment_id=segment_id)\n",
    "                            if not df.empty:\n",
    "                                buckets[k].append(df)\n",
    "                    continue\n",
    "\n",
    "                # sampled signals\n",
    "                segments = payload if isinstance(payload, list) else [payload]\n",
    "                for seg in segments:\n",
    "                    if not isinstance(seg, dict):\n",
    "                        continue\n",
    "                    if \"timestampStart\" not in seg or \"samplingFrequency\" not in seg:\n",
    "                        continue\n",
    "                    df = segment_to_df(seg, signal_name=k, segment_id=segment_id)\n",
    "                    if not df.empty:\n",
    "                        buckets[k].append(df)\n",
    "\n",
    "    out = {}\n",
    "    for k, dfs in buckets.items():\n",
    "        if not dfs:\n",
    "            out[k] = pd.DataFrame(columns=[\"timestamp_ns\", \"datetime_utc\", \"segment\"])\n",
    "            print(f\"[WARN] No data for {k}\")\n",
    "            continue\n",
    "\n",
    "        df_all = pd.concat(dfs, ignore_index=True)\n",
    "        df_all = df_all.sort_values([\"timestamp_ns\", \"segment\"], kind=\"mergesort\")\n",
    "        # Overlap handling within same timestamp: keep last by (timestamp, segment order)\n",
    "        df_all = df_all.drop_duplicates(subset=[\"timestamp_ns\"], keep=\"last\")\n",
    "\n",
    "        df_all.insert(1, \"datetime_utc\", pd.to_datetime(df_all[\"timestamp_ns\"], unit=\"ns\", utc=True))\n",
    "        out[k] = df_all.reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def write_channel_csvs(channel_dfs: dict, out_dir: str, prefix: str):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for k, df in channel_dfs.items():\n",
    "        out_path = os.path.join(out_dir, f\"{prefix}_{k}.csv\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"[OK] wrote {k}: {out_path} ({len(df)} rows)\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Part 2 (NOT IMPLEMENTED YET): unified 64Hz grid, HR derivation, sections, cross-segment interpolation\n",
    "# ----------------------------\n",
    "def merge_to_64hz_sections(channel_dfs: dict, gap_threshold_s: float = 60.0):\n",
    "    \"\"\"\n",
    "    TODO (do NOT implement yet per your request):\n",
    "      - for each segment: choose a 64Hz grid (likely from BVP timestamps)\n",
    "      - map EDA/TEMP/ACC to that grid (nearest-snap OR interpolation)\n",
    "      - derive per-second HR from systolic peaks (filter -> interpolate), then upsample/interp to 64Hz grid\n",
    "      - merge channels into one DF per segment\n",
    "      - concatenate segments; decide 'section' based on inter-segment gap > threshold; optionally interpolate across short gaps\n",
    "      - handle macro-grid phase mismatch between segments\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Planned; not coded yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/001/001_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/001/001_eda.csv (7328 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/001/001_temperature.csv (1832 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/001/001_bvp.csv (117120 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/001/001_systolicPeaks.csv (2119 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/002/002_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/002/002_eda.csv (7052 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/002/002_temperature.csv (1763 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/002/002_bvp.csv (110528 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/002/002_systolicPeaks.csv (2404 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/003/003_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/003/003_eda.csv (8608 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/003/003_temperature.csv (2152 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/003/003_bvp.csv (137152 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/003/003_systolicPeaks.csv (2553 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/004/004_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/004/004_eda.csv (12684 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/004/004_temperature.csv (3171 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/004/004_bvp.csv (202688 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/004/004_systolicPeaks.csv (4370 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/005/005_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/005/005_eda.csv (5996 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/005/005_temperature.csv (1499 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/005/005_bvp.csv (95872 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/005/005_systolicPeaks.csv (1857 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/006/006_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/006/006_eda.csv (5900 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/006/006_temperature.csv (1475 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/006/006_bvp.csv (94080 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/006/006_systolicPeaks.csv (1736 rows)\n",
      "[WARN] No data for accelerometer\n",
      "[OK] wrote accelerometer: ./extraction_from_original_files/007/007_accelerometer.csv (0 rows)\n",
      "[OK] wrote eda: ./extraction_from_original_files/007/007_eda.csv (13300 rows)\n",
      "[OK] wrote temperature: ./extraction_from_original_files/007/007_temperature.csv (3324 rows)\n",
      "[OK] wrote bvp: ./extraction_from_original_files/007/007_bvp.csv (212544 rows)\n",
      "[OK] wrote systolicPeaks: ./extraction_from_original_files/007/007_systolicPeaks.csv (4221 rows)\n"
     ]
    }
   ],
   "source": [
    "for subject_id in range(1, 8):\n",
    "    write_channel_csvs(parse_folder_to_channel_dfs(f'./original_files/physiological_signal/00{subject_id}-3YK9K1J2D2/raw_data/v6'), out_dir=f'./extraction_from_original_files/00{subject_id}', prefix=f'00{subject_id}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
